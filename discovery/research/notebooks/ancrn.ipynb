{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Audio Exploration: Characteristics, Distribution, Frequency and Spectrograms:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74c0ff4f7caa7def"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import librosa\n",
    "from librosa.display import waveshow, specshow, cmap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c3de48366c973e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"./dataset/batches\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26226668fe3e43d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# random files for research\n",
    "def get_samples_path(data_directory, n_samples):\n",
    "    if n_samples == 1:\n",
    "        return [Path(data_directory, file) for file in np.random.choice(os.listdir(data_directory), n_samples)][0]\n",
    "    return [Path(data_directory, file) for file in np.random.choice(os.listdir(data_directory), n_samples)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a01f626cefb7b95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get random files for exploration\n",
    "sample_paths = get_samples_path(data_directory = DATA_DIR, n_samples = 300)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aea7f68c0fd03399"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_audio_stats(audio_paths):\n",
    "    audio_stats = []\n",
    "    for audio in audio_paths:\n",
    "        signal, sr = librosa.load(audio, sr = None, mono = False)\n",
    "        # Channel determination\n",
    "        channels = \"stereo\" if len(signal.shape) > 1 and signal.shape[0] == 2 else \"mono\"\n",
    "        duration = librosa.get_duration(y=signal, sr = sr)\n",
    "        mean_amplitude = np.mean(np.abs(signal))\n",
    "        median_amplitude = np.median(np.abs(signal))\n",
    "        std_amplitude = np.std(signal)\n",
    "        # Root-Mean-Square-Energy\n",
    "        rmse = np.mean(librosa.feature.rms(y=signal))\n",
    "        # Average Zero crossing rate\n",
    "        zcr = np.mean(librosa.zero_crossings(y=signal, pad = False)) / len(signal)\n",
    "        \n",
    "        audio_stats.append( \n",
    "            { \"file_name\": audio.parts[-1],\n",
    "              \"sample_rate\" : sr,\n",
    "              \"duration\": duration,\n",
    "              \"channels\": channels,\n",
    "              \"root_mean_square_energy\": rmse,\n",
    "              \"zero_crossing_rate_avg\": zcr,\n",
    "              \"mean_amplitude\": mean_amplitude,\n",
    "              \"median_amplitude\": median_amplitude,\n",
    "              \"std_amplitude\": std_amplitude})\n",
    "    \n",
    "    return pd.DataFrame(audio_stats)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69f0a7db344010d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats = get_audio_stats(sample_paths)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e70f1c96de6f9fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91b28d173a8d51c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, constrained_layout = True)\n",
    "\n",
    "plt.subplot(2, 2, 1) \n",
    "plt.hist(stats.root_mean_square_energy)\n",
    "plt.title(\"Root-Mean-Square-Energy\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"zcr\")\n",
    "\n",
    "plt.subplot(2, 2, 2) \n",
    "plt.hist(stats.zero_crossing_rate_avg)\n",
    "plt.title(\"Zero-Crossing-Rate, Avg.\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"zcr\")\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 3) \n",
    "plt.hist(stats.mean_amplitude)\n",
    "plt.title(\"Mean Amplitude\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Mean Amplitude, Hz\")\n",
    "\n",
    "plt.subplot(2, 2, 4) \n",
    "plt.hist(stats.std_amplitude)\n",
    "plt.title(\"Std Amplitude\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Median Amplitude, Hz\")\n",
    "plt.show();\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d68049ff1b271b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    " If all the files are from a very similar environment (so we have), it does make sense that the statistics across the audio samples would be quite consistent. This is especially true if the driving conditions, vehicle, and recording equipment were roughly the same across all recordings. **But nevertheless it is the strangest pattern in data in my experience lol, especially working with audio files.\n",
    " \n",
    "FYI: it's essential to be aware that while this consistency can be beneficial for training a model (**as it reduces variability**), it's crucial to ensure that the model generalizes well to other environments or variations when deploying in real-world scenarios...\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49608b52329dc7af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "signal, sr = librosa.load(sample_paths[2], sr = None)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(signal, sr=sr)\n",
    "plt.title(\"Waveform of a Segment from the First Audio Sample\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4710de149d308f4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the spectrogram\n",
    "D = librosa.amplitude_to_db(librosa.stft(signal), ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram of Audio Sample')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1d2de663705b34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing (draft):\n",
    "\n",
    "* Convert stereo to mono (in any case)\n",
    "* Normalize the audio volume levels\n",
    "* Extract real and imaginary spectrograms for each audio file\n",
    "* Silence segmentation (?)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da5d3b8da4ab6988"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_path):\n",
    "    \"\"\"\n",
    "    Preprocessing of an audio file:\n",
    "    - Converts stereo to mono\n",
    "    - Normalize the volume\n",
    "    - Extract the real and imaginary spectrograms\n",
    "    \n",
    "    :param audio_path: the path to the audio file to read \n",
    "    :return: \n",
    "        - Real part of the spectrogram\n",
    "        - Imaginary part of the spectrogram\n",
    "    \"\"\"\n",
    "    # load audio\n",
    "    y, sr = librosa.load(audio_path, sr = None, mono = True)\n",
    "    # Normalize \n",
    "    y = y / np.max(np.abs(y))\n",
    "    # Compute STFT to get the complex spectrogram\n",
    "    complex_spectrogram = librosa.stft(y)\n",
    "    \n",
    "    # Split the complex spectrogram into real and imaginary parts\n",
    "    real_spectrogram = np.real(complex_spectrogram)\n",
    "    imag_spectrogram = np.imag(complex_spectrogram)\n",
    "    \n",
    "    return sr, real_spectrogram, imag_spectrogram"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd40a394b2174a16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def detect_silent_segments(audio_path, threshold=0.01, frame_length=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Detect silent segments in an audio file. Updated to handle both mono and stereo audio and\n",
    "    audio lengths that aren't exact multiples of the hop length.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_path: Path to the audio file.\n",
    "    - threshold: Amplitude threshold below which audio is considered silent.\n",
    "    - frame_length: Number of audio samples between successive frames.\n",
    "    - hop_length: Number of audio samples between starts of consecutive frames.\n",
    "    \n",
    "    Returns:\n",
    "    - silent_segments: List of start and end times for silent segments.\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(audio_path, sr=None, mono=False)\n",
    "    \n",
    "    # If stereo, average the two channels\n",
    "    if len(y.shape) > 1 and y.shape[0] == 2:\n",
    "        y = np.mean(y, axis=0)\n",
    "        \n",
    "    amplitude = np.abs(y)\n",
    "    \n",
    "    # Adjust length of amplitude array for reshaping\n",
    "    num_segments = len(amplitude) // hop_length\n",
    "    adjusted_length = num_segments * hop_length\n",
    "    amplitude_adjusted = amplitude[:adjusted_length]\n",
    "    \n",
    "    # Detect silent frames\n",
    "    silent_frames = np.where(np.mean(amplitude_adjusted.reshape(-1, hop_length), axis=1) < threshold)[0]\n",
    "    \n",
    "    silent_segments = []\n",
    "    \n",
    "    if len(silent_frames) > 0:\n",
    "        current_segment = [silent_frames[0]]\n",
    "        for i in range(1, len(silent_frames)):\n",
    "            if silent_frames[i] != silent_frames[i-1] + 1:\n",
    "                current_segment.append(silent_frames[i-1])\n",
    "                silent_segments.append(current_segment)\n",
    "                current_segment = [silent_frames[i]]\n",
    "        current_segment.append(silent_frames[-1])\n",
    "        silent_segments.append(current_segment)\n",
    "    \n",
    "    # Convert frame numbers to time\n",
    "    silent_segments = [(librosa.frames_to_time(seg[0], sr=sr, hop_length=hop_length),\n",
    "                        librosa.frames_to_time(seg[1], sr=sr, hop_length=hop_length)) for seg in silent_segments]\n",
    "    \n",
    "    return silent_segments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5958e59babf7a1d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Detecting silent segments in the provided audio samples using the updated function\n",
    "from pprint import pprint\n",
    "silent_segments_info = {}\n",
    "\n",
    "for file in sample_paths:\n",
    "    silent_segments = detect_silent_segments(file)\n",
    "    if silent_segments:\n",
    "        silent_segments_info[file.parts[-1]] = silent_segments\n",
    "\n",
    "pprint(silent_segments_info)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45242ba30aa1db2f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As there is a small length of segments it is not mandatory to preprocess and remove them (but it is better to see performance of the model with and without them). Just my thoughts. \n",
    "\n",
    " In general, I'd recommend keeping the entire audio unless the silent segments are significantly long, as these brief pauses could be inherent characteristics of the environment and might provide context during noise cancellation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "696f0ee0863bd26b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sr, real, imag = preprocess_audio(sample_paths[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a29778e666bae340"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display the real and imaginary spectrogram for the first audio sample\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "librosa.display.specshow(real, sr=sr, x_axis='time', y_axis='linear')\n",
    "plt.colorbar()\n",
    "plt.title(\"Real Part of Spectrogram\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "librosa.display.specshow(imag, sr=sr, x_axis='time', y_axis='linear')\n",
    "plt.colorbar()\n",
    "plt.title(\"Imaginary Part of Spectrogram\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbb69d0d08f4c56f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Preprocessing class for module \n",
    "class AudioPreprocessor:\n",
    "    def __init__(self, n_fft=2048, hop_length=512, segment_length=44100):\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.segment_length = segment_length\n",
    "\n",
    "    def get_spectrogram(self, y):\n",
    "        spectrogram = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=self.n_fft, hop_length=self.hop_length)), ref=np.max)\n",
    "        return spectrogram\n",
    "\n",
    "    def normalize_spectrogram(self, spectrogram):\n",
    "        mean = np.mean(spectrogram)\n",
    "        std = np.std(spectrogram)\n",
    "        return (spectrogram - mean) / std\n",
    "\n",
    "    def pitch_shift(self, y, sr, steps=2):\n",
    "        return librosa.effects.pitch_shift(y, sr, n_steps=steps)\n",
    "\n",
    "    def add_noise(self, y, noise_level=0.005):\n",
    "        noise = np.random.normal(size=y.shape)\n",
    "        return y + noise_level * noise\n",
    "\n",
    "    def segment_audio(self, y):\n",
    "        num_segments = len(y) // self.segment_length\n",
    "        segments = [y[i * self.segment_length: (i + 1) * self.segment_length] for i in range(num_segments)]\n",
    "        return segments\n",
    "\n",
    "    def preprocess(self, audio_path, augment=False, noise_level=0.005, pitch_steps=2):\n",
    "        y, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "        \n",
    "        if augment:\n",
    "            y = self.add_noise(y, noise_level)\n",
    "            y = self.pitch_shift(y, sr, pitch_steps)\n",
    "        \n",
    "        segments = self.segment_audio(y)\n",
    "        spectrograms = [self.get_spectrogram(segment) for segment in segments]\n",
    "        normalized_spectrograms = [self.normalize_spectrogram(spectrogram) for spectrogram in spectrograms]\n",
    "        \n",
    "        return normalized_spectrograms"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "854ef2def7cf73e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model's architecture: Deep Concolutional Neural Network:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aafc1096d5ee790a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "class ANCRN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANCRN, self).__init__()\n",
    "        \n",
    "        # Encoder part\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # LSTM middle part\n",
    "        self.lstm = nn.LSTM(input_size = 256, hidden_size = 128, num_layers = 2, batch_first = True)\n",
    "        \n",
    "        # Decoder part with transposed conolutions\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size = 2, stride = 2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(256, 64, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 32, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 16, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder part\n",
    "        encoder_outputs = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            encoder_outputs.append(x)\n",
    "        \n",
    "        # LSTM middle part\n",
    "        print(x.shape)\n",
    "        x = x.squeeze(-1).permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x  = x.permute(0, 2, 1).unsqueeze(-1)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        for i, layer in enumerate(self.decoder):\n",
    "            if isinstance(layer, nn.ConvTranspose2d):\n",
    "                x = torch.cat([x, encoder_outputs[-(1//3)-1]], dim = 1)\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d59e26e7812943da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model_summary(model):\n",
    "    print(\"Model Summary\")\n",
    "    print(\"--------------\")\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        num_params = parameter.numel()\n",
    "        total_params += num_params\n",
    "        print(f\"{name.ljust(25)}: {str(num_params).ljust(12)} parameters\")\n",
    "    print(\"--------------\")\n",
    "    print(f\"Total Parameters: {total_params}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41ff3ebf5cc5ccd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_instance = ANCRN()\n",
    "model_instance.parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c7fde4d450f7779"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_summary(model_instance)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f9a975b5d521212"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del(model_instance)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eef268f74c736f9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Flatten the batch since each audio file might produce multiple segments (spectrograms)\n",
    "    all_spectrograms = [spectrogram for segments in batch for spectrogram in segments]\n",
    "    \n",
    "    # Find the maximum width (time dimension) among the spectrograms\n",
    "    max_width = max([item.shape[1] for item in all_spectrograms])\n",
    "    \n",
    "    # Create a placeholder tensor filled with zeros, with the shape of the largest spectrogram\n",
    "    padded_batch = torch.zeros((len(all_spectrograms), all_spectrograms[0].shape[0], max_width))\n",
    "    \n",
    "    # Pad each spectrogram and place it in the placeholder tensor\n",
    "    for i, item in enumerate(all_spectrograms):\n",
    "        padded_batch[i, :, :item.shape[1]] = torch.tensor(item)\n",
    "\n",
    "    return torch.unsqueeze(padded_batch, 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa40b22f01778864"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, preprocessor):\n",
    "        self.audio_files = audio_files\n",
    "        self.preprocessor = preprocessor\n",
    "        self.data = [self.preprocessor.preprocess(file) for file in audio_files]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Instantiate the AudioPreprocessor\n",
    "audio_preprocessor = AudioPreprocessor()\n",
    "dataset = AudioDataset(sample_paths, audio_preprocessor)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77a483050f388c25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd8bb84f82104294"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3527d1c60e3c175e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "model = ANCRN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr = 0.001) # using by default (without custom hyperparameters)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4940dc17a924923"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 10 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    # Training\n",
    "    for batch in train_loader:\n",
    "        inputs = batch.to(device)\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_loader:\n",
    "            val_inputs = val_batch.to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss = criterion(val_outputs, val_inputs)\n",
    "            total_val_loss += val_loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {avg_val_loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "485d450ebbc9a27b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del(model)\n",
    "del(loss)\n",
    "del(optimizer)\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "484ee228ec46e97c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for test_batch in test_loader:\n",
    "        test_inputs = test_batch.to(device)\n",
    "        test_outputs = model(test_inputs)\n",
    "        test_loss = criterion(test_outputs, test_inputs)\n",
    "        total_test_loss += test_loss.item()\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_loader)\n",
    "print(f\"Average Test Loss: {avg_test_loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d47e677a5d7fdd29"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
